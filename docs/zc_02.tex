\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{pythontex}

\title{TensorGame and TensorState Overview}
\author{JAX Spiel Team}
\date{\today}

\begin{document}
\maketitle

\begin{pycode}
import json
from pathlib import Path

json_path = Path("zc02/algebra/ZC02_payoffs.json")
with json_path.open() as f:
    payoff_data = json.load(f)

u_row_expanded = payoff_data["u_row"]["expanded"]
u_col_expanded = payoff_data["u_col"]["expanded"]
u_row_factored = payoff_data["u_row"]["factored"]
u_col_factored = payoff_data["u_col"]["factored"]
\end{pycode}

\section{TensorGame Roadmap}
The TensorGame abstraction follows the global milestones outlined in the JAX Spiel conversion plan. Each milestone informs how we
stabilize immutable game definitions, ensure batched utilities, and progressively extend to richer gameplay scenarios.
\input{zc02/milestones.tex}

\section{Algebraic Payoff Summary}
Mixed strategies assign the row player a top-action probability $p$ and the column player a left-action probability $q$. Given row
payoff symbols $r_{ij}$ and column payoff symbols $c_{ij}$, the expected utilities derived in
\texttt{docs/zc02/algebra/ZC02\_payoffs.ipynb} are synchronized below via PythonTeX.

\subsection{Expanded forms}
\begin{align*}
u_{\text{row}} &= \py{u_row_expanded},\\
u_{\text{col}} &= \py{u_col_expanded}.
\end{align*}

\subsection{Factored forms}
\begin{align*}
u_{\text{row}} &= \py{u_row_factored},\\
u_{\text{col}} &= \py{u_col_factored}.
\end{align*}

\section{TensorState Integration Milestones}
TensorState development mirrors the same milestones with a focus on state evolution. Core API work guarantees that tensor-backed
state transitions remain pure and differentiable. Example game implementations validate that TensorState snapshots capture all
player information. Policy and value function milestones bind TensorState with evaluators, while benchmarking commitments ensure
state rollouts remain reliable under pytest parity with OpenSpiel.

\section{Policy Evaluation Trajectory}
Milestone 3 of the conversion plan emphasizes policy evaluation through differentiable objectives. We evaluate the policy parameters $\theta$ using the inline objective
\begin{equation*}
  J(\theta) = \mathbb{E}_{s \sim d^{\pi_\theta},\, a \sim \pi_\theta}\bigl[r(s, a)\bigr],
\end{equation*}
where $d^{\pi_\theta}$ is the stationary distribution induced by \(\pi_\theta\). The gradient-based update therefore respects JAX transformations via
\begin{equation*}
  \nabla_\theta J(\theta) = \mathbb{E}_{s, a}\bigl[\nabla_\theta \log \pi_\theta(a \mid s)\, r(s, a)\bigr],
\end{equation*}
pinning the policy learning narrative directly to plan milestone 3.

\section{Value Function Coupling}
The same milestone mandates value propagation that complements policy gradients. We track state values through
\begin{equation*}
  v^{\pi}(s) = \sum_a \pi(a \mid s) q^{\pi}(s, a),
\end{equation*}
and relate action values to temporal rollouts by
\begin{equation*}
  q^{\pi}(s, a) = r(s, a) + \gamma \sum_{s'} P(s' \mid s, a) v^{\pi}(s').
\end{equation*}
These expressions align the TensorState snapshots with differentiable evaluators that can be vmapped or jitted for batched computation.

\begin{figure}[t]
  \centering
  \input{zc02/figures/policy_value_alignment.tex}
  \caption{Milestone 3 policy/value alignment between the conversion plan and TensorState objectives.}
  \label{fig:policy-value-alignment}
\end{figure}

\section{Documentation Alignment}
This document is automatically synchronized with the strategic plan. Run
\texttt{python docs/zc02/generate\_milestones.py} whenever \texttt{docs/plan.md} changes to refresh the milestone summary before
building the PDF with \texttt{latexmk -pdf docs/zc\_02.tex}.

\end{document}
